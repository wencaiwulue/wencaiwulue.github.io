# åä¸ºæ˜‡è…¾ 910b3 å•æœºå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒæŠ¥é”™å¤ç°

![img_2.png](img_2.png)

## åœ¨ä¸€ä¸ª node ä¸Šå¯åŠ¨ä¸¤ä¸ªå®¹å™¨

é•œåƒï¼šswr.cn-east-317.qdrgznjszx.com/donggang/llama-factory-ascend910b:cann8-py310-torch2.2.0-ubuntu18.04
master.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cni.projectcalico.org/containerID: 283ca17afc61b98aa5bd574c186720caa94f70717e34bf483d4d2e259a7ee65f
    cni.projectcalico.org/podIP: 10.233.97.242/32
    cni.projectcalico.org/podIPs: 10.233.97.242/32
    hami.io/Ascend910B-devices-allocated: Ascend910B-0,Ascend910B,65536,0:;
    hami.io/Ascend910B-devices-to-allocate: Ascend910B-0,Ascend910B,65536,0:;
    hami.io/bind-phase: allocating
    hami.io/bind-time: "1739785211"
    hami.io/vgpu-node: 10.56.118.17
    hami.io/vgpu-time: "1739785211"
    huawei.com/Ascend910B: '[{"UUID":"Ascend910B-0"}]'
    nvidia.com/use-gputype: Ascend910B
    predicate-time: "1739785211"
    scheduling.k8s.io/group-name: ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-ca64c0e1-0aea-43e9-b94b-69acf80400da
  labels:
    hami.io/vgpu-node: 10.56.118.17
    maas.epscp.volcengine.com/env: smoke
    maas.epscp.volcengine.com/exec-id: job-exec-id-cuhjsp6vdf5qkivmfhr0
    maas.epscp.volcengine.com/job-id: job-id-cuhhmtoj6nrl6so7i5gg
    maas.epscp.volcengine.com/role: master
  name: ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-master-0
  namespace: maas-task
spec:
  containers:
    - args:
        - tail
        - -f
        - /dev/null
      env:
        - name: HCCL_EXEC_TIMEOUT
          value: "1800"
      image: swr.cn-east-317.qdrgznjszx.com/donggang/llama-factory-ascend910b:cann8-py310-torch2.2.0-ubuntu18.04
      imagePullPolicy: Always
      name: train
      resources:
        limits:
          cpu: "5"
          huawei.com/Ascend910B: "1"
          huawei.com/Ascend910B-memory: "65536"
          memory: 20Gi
        requests:
          cpu: "5"
          huawei.com/Ascend910B: "1"
          huawei.com/Ascend910B-memory: "65536"
          memory: 20Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: memory-share
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: 10.56.118.17
  nodeSelector:
    resourcepool.aicp.epscp.volcengine.com/id: cu4jr2vm53m8knmfoljg
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  volumes:
    - emptyDir:
        medium: Memory
      name: memory-share
```
```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cni.projectcalico.org/containerID: 9e7b1e2bcf20dd4b8e322b2f52738db0431f91144136c9cd91a974a926b279e4
    cni.projectcalico.org/podIP: 10.233.97.20/32
    cni.projectcalico.org/podIPs: 10.233.97.20/32
    hami.io/Ascend910B-devices-allocated: Ascend910B-3,Ascend910B,65536,0:;
    hami.io/Ascend910B-devices-to-allocate: Ascend910B-3,Ascend910B,65536,0:;
    hami.io/bind-phase: allocating
    hami.io/bind-time: "1739785209"
    hami.io/vgpu-node: 10.56.118.17
    hami.io/vgpu-time: "1739785209"
    huawei.com/Ascend910B: '[{"UUID":"Ascend910B-3"}]'
    nvidia.com/use-gputype: Ascend910B
    predicate-time: "1739785209"
    scheduling.k8s.io/group-name: ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-ca64c0e1-0aea-43e9-b94b-69acf80400da
  labels:
    hami.io/vgpu-node: 10.56.118.17
    maas.epscp.volcengine.com/env: smoke
    maas.epscp.volcengine.com/exec-id: job-exec-id-cuhjsp6vdf5qkivmfhr0
    maas.epscp.volcengine.com/job-id: job-id-cuhhmtoj6nrl6so7i5gg
    maas.epscp.volcengine.com/role: worker
    maas.epscp.volcengine.com/role-index: "0"
  name: ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-worker-0-0
  namespace: maas-task
spec:
  containers:
    - args:
        - tail
        - -f
        - /dev/null
      env:
        - name: HCCL_EXEC_TIMEOUT
          value: "1800"
      image: swr.cn-east-317.qdrgznjszx.com/donggang/llama-factory-ascend910b:cann8-py310-torch2.2.0-ubuntu18.04
      imagePullPolicy: Always
      name: train
      resources:
        limits:
          cpu: "5"
          huawei.com/Ascend910B: "1"
          huawei.com/Ascend910B-memory: "65536"
          memory: 20Gi
        requests:
          cpu: "5"
          huawei.com/Ascend910B: "1"
          huawei.com/Ascend910B-memory: "65536"
          memory: 20Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /dev/shm
          name: memory-share
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: 10.56.118.17
  nodeSelector:
    resourcepool.aicp.epscp.volcengine.com/id: cu4jr2vm53m8knmfoljg
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  schedulerName: xgpu-scheduler
  securityContext: { }
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 600
  volumes:
    - emptyDir:
        medium: Memory
      name: memory-share
```

## ä¸‹è½½æ¨¡å‹ Qwen2-0.5B-Instruct

https://huggingface.co/Qwen/Qwen2-0.5B-Instruct

```shell
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download Qwen/Qwen2-0.5B-Instruct --local-dir /mnt/model/base_model
```

## å°†å¦‚ä¸‹ json æ–‡ä»¶ä¿å­˜ä¸º /mnt/data/train/train.jsonl

```text
{"text": "ç¬¬ä¸€ç« è®º"}
{"text": "ä¼ æŸ“ç—…æ˜¯æŒ‡ç”±ç—…åŸå¾®ç”Ÿç‰©ï¼Œå¦‚æœŠç²’ã€ç—…æ¯’ã€è¡£åŸä½“ã€ç«‹å…‹æ¬¡ä½“ã€æ”¯åŸä½“ï¼ˆmycoplasma)ç»†èŒçœŸèŒã€èºæ—‹ä½“å’Œå¯„ç”Ÿè™«ï¼Œå¦‚åŸè™«ã€è •è™«ã€åŒ»å­¦æ˜†è™«æ„ŸæŸ“äººä½“åäº§ç”Ÿçš„æœ‰ä¼ æŸ“æ€§ã€åœ¨ä¸€å®šæ¡ä»¶ä¸‹å¯é€ æˆæµè¡Œçš„ç–¾ç—…ã€‚æ„ŸæŸ“æ€§ç–¾ç—…æ˜¯æŒ‡ç”±ç—…åŸä½“æ„ŸæŸ“æ‰€è‡´çš„ç–¾ç—…ï¼ŒåŒ…æ‹¬ä¼ æŸ“ç—…å’Œéä¼ æŸ“æ€§æ„ŸæŸ“æ€§ç–¾ç—…ã€‚"}
{"text": "ä¼ æŸ“ç—…å­¦æ˜¯ä¸€é—¨ç ”ç©¶å„ç§ä¼ æŸ“ç—…åœ¨äººä½“å†…å¤–å‘ç”Ÿã€å‘å±•ã€ä¼ æ’­ã€è¯Šæ–­ã€æ²»ç–—å’Œé¢„é˜²è§„å¾‹çš„å­¦ç§‘ã€‚é‡ç‚¹ç ”ç©¶å„ç§ä¼ æŸ“ç—…çš„å‘ç—…æœºåˆ¶ã€ä¸´åºŠè¡¨ç°ã€è¯Šæ–­å’Œæ²»ç–—æ–¹æ³•ï¼ŒåŒæ—¶å…¼é¡¾æµè¡Œç—…å­¦å’Œé¢„é˜²æªæ–½çš„ç ”ç©¶ï¼Œåšåˆ°é˜²æ²»ç»“åˆã€‚"}
{"text": "ä¼ æŸ“ç—…å­¦ä¸å…¶ä»–å­¦ç§‘æœ‰å¯†åˆ‡è”ç³»ï¼Œå…¶åŸºç¡€å­¦ç§‘å’Œç›¸å…³å­¦ç§‘åŒ…æ‹¬ç—…åŸç”Ÿç‰©å­¦ã€åˆ†å­ç”Ÿç‰©å­¦ã€å…ç–«å­¦ã€äººä½“å¯„ç”Ÿè™«å­¦ã€æµè¡Œç—…å­¦ã€ç—…ç†å­¦ã€è¯ç†å­¦å’Œè¯Šæ–­å­¦ç­‰ã€‚æŒæ¡è¿™äº›å­¦ç§‘çš„åŸºæœ¬çŸ¥è¯†ã€åŸºæœ¬ç†è®ºå’ŒåŸºæœ¬æŠ€èƒ½å¯¹å­¦å¥½ä¼ æŸ“ç—…å­¦èµ·ç€éå¸¸é‡è¦çš„ä½œç”¨ã€‚"}
{"text": "åœ¨äººç±»å†å²é•¿æ²³ä¸­ï¼Œä¼ æŸ“ç—…ä¸ä»…å¨èƒç€äººç±»çš„å¥åº·å’Œç”Ÿå‘½ï¼Œè€Œä¸”å½±å“ç€äººç±»æ–‡æ˜çš„è¿›ç¨‹ï¼Œç”šè‡³æ”¹å†™è¿‡äººç±»å†å²ã€‚äººç±»åœ¨ä¸ä¼ æŸ“ç—…è¾ƒé‡è¿‡ç¨‹ä¸­ï¼Œå–å¾—äº†è®¸å¤šé‡å¤§æˆ˜æœï¼Œ19ä¸–çºªä»¥æ¥ï¼Œç—…åŸå¾®ç”Ÿç‰©çš„ä¸æ–­å‘ç°åŠå…¶åˆ†å­ç”Ÿç‰©å­¦çš„å…´èµ·ï¼Œæ¨åŠ¨äº†ç”Ÿå‘½ç§‘å­¦ä¹ƒè‡³æ•´ä¸ªåŒ»å­¦çš„å‘å±•ï¼›ç–«è‹—çš„ç ”ç©¶è¯ç”Ÿäº†æ„ŸæŸ“å…ç–«å­¦ï¼Œå¥ å®šäº†å…ç–«å­¦çš„ç†è®ºåŸºç¡€ï¼Œå·²ç”¨æ¥ç ”ç©¶å„ç§ç–¾ç—…çš„å‘ç”Ÿæœºåˆ¶åŠé˜²æ²»æ‰‹æ®µï¼›æŠ—ç”Ÿç´ çš„å‘ç°å’Œåº”ç”¨è¢«èª‰ä¸º20ä¸–çºªæœ€ä¼Ÿå¤§çš„åŒ»å­¦æˆå°±ï¼›â€œKochæ³•åˆ™â€œæ˜ç¡®äº†ä¼ æŸ“ç—…ä¸ç—…åŸå¾®ç”Ÿç‰©ä¹‹é—´çš„å› æœå…³ç³»ï¼Œå»ºç«‹äº†ç—…åŸå­¦ç†è®ºï¼Œå·²è¢«å¹¿æ³›åº”ç”¨åˆ°å…¶ä»–è®¸å¤šç–¾ç—…çš„ç ”ç©¶ï¼Œå¥ å®šäº†ç°ä»£åŒ»å­¦å‘å±•çš„åŸºçŸ³ã€‚"}
{"text": "æ­£æ˜¯ç”±äºä¸Šè¿°è¾‰ç…Œæˆ˜æœï¼ŒåŠ ä¸Šç¤¾ä¼šæ–‡æ˜çš„æ¨è¿›å’Œç‰©è´¨ç”Ÿæ´»æ°´å¹³çš„æé«˜ï¼Œäººç±»é€æ¸åœ¨ä¸ä¼ æŸ“ç—…çš„æ–—äº‰ä¸­å äº†ä¸Šé£ã€‚20ä¸–çºª70å¹´ä»£è¥¿æ–¹åŒ»å­¦ç•Œä¸€åº¦è®¤ä¸ºï¼Œä¼ æŸ“ç—…æ­£åœ¨æ¶ˆäº¡ã€‚ç„¶è€Œï¼Œ1981å¹´çš„è‰¾æ»‹ç—…ã€2003å¹´çš„ä¼ æŸ“æ€§éå…¸å‹è‚ºç‚ã€2012å¹´çš„ä¸­ä¸œå‘¼å¸ç»¼åˆå¾ã€2013å¹´çš„äººæ„ŸæŸ“H7N9ç¦½æµæ„Ÿã€2014å¹´çš„åŸƒåšæ‹‰å‡ºè¡€çƒ­ç­‰æ–°çš„ä¼ æŸ“ç—…ç›¸ç»§å‡ºç°ï¼Œä¸æ–­ç»™äººç±»æ•²å“è­¦é’Ÿï¼›ä¸æ­¤åŒæ—¶ï¼Œç™»é©çƒ­ã€ç»“æ ¸ç—…ã€ç—‡ç–¾åŠæ€§ä¼ æ’­ç–¾ç—…ç­‰è€ä¼ æŸ“ç—…å†åº¦è‚†è™ï¼Œä¸¥é‡å½±å“ä¸–ç•Œç»æµå‘å±•å’Œç¤¾ä¼šå’Œè°ã€‚20ä¸–çºª90å¹´ä»£å›½é™…ä¸Šæå‡ºäº†â€œeme1ä¸€ging infectiou s diseases\"çš„æ¦‚å¿µï¼Œèµ·åˆè¢«æˆ‘å›½å­¦è€…ç¿»è¯‘ä¸ºâ€œæ–°å‘ä¼ æŸ“ç—…â€ï¼Œæ­¤åéšç€äººä»¬å¯¹æ„ŸæŸ“æ€§ç–¾ç—…è®¤è¯†çš„ä¸æ–­æ·±å…¥ï¼Œè¯¥å®šä¹‰å¾—åˆ°äº†ä¿®è®¢ï¼Œâ€œæ–°å‘ä¼ æŸ“ç—…â€é€æ¸æ¼”å˜ä¸ºâ€œæ–°å‘æ„ŸæŸ“ç—…â€ï¼Œä¸ä»…åŒ…æ‹¬ç”±æ–°ç§æˆ–æ–°å‹ç—…åŸå¾®ç”Ÿç‰©å¼•èµ·çš„æ–°å‘ç°çš„æ„ŸæŸ“ç—…ï¼Œè€Œä¸”åŒ…æ‹¬è¿‘å¹´æ¥å¯¼è‡´åœ°åŒºæ€§æˆ–å›½é™…æ€§å…¬å…±å«ç”Ÿé—®é¢˜çš„å†å‘çš„è€æ„ŸæŸ“ç—…ã€‚æ–°ä¼ æŸ“ç—…çš„å‡ºç°ï¼Œè€ä¼ æŸ“ç—…çš„å¤ç‡ƒï¼Œç—…åŸä½“å¯¹æŠ—èŒè¯ç‰©è€è¯æ€§çš„å¢åŠ ï¼Œæ„æˆäº†å¯¹äººç±»å¥åº·çš„å·¨å¤§å¨èƒã€‚ç›®å‰ï¼Œä¸–ç•Œå«ç”Ÿç»„ç»‡åŠå„å›½æ”¿åºœå‡é«˜åº¦é‡è§†ä¼ æŸ“ç—…é˜²æ§å·¥ä½œï¼Œä¸æ–­æ¨å‡ºå…¨çƒæ€§çš„ç–¾ç—…è¯Šæ–­å’ŒæŒ‡å—ï¼Œå¹¶ä½¿å¾—ä¼ æŸ“ç—…ç ”ç©¶å·¥ä½œæ›´å®¹æ˜“å¾—åˆ°è·¨åœ°åŒºã€è·¨éƒ¨é—¨ã€è·¨é¢†åŸŸçš„åˆä½œï¼Œç ”ç©¶æˆæœä¹Ÿèƒ½æ›´å¿«åœ°å¾—åˆ°å…¨çƒåˆ†äº«ã€‚"}
{"text": "æ–°ä¸­å›½æˆç«‹åï¼Œåœ¨â€œé¢„é˜²ä¸ºä¸»ã€é˜²æ²»ç»“åˆâ€çš„å«ç”Ÿæ–¹é’ˆæŒ‡å¼•ä¸‹ï¼Œå«ç”Ÿæ¡ä»¶æ˜æ˜¾æ”¹å–„ï¼ŒåŒ»è¯æ°´å¹³å¤§å¹…æé«˜ï¼Œå›´ç”ŸæœŸä¿å¥å·¥ä½œä¸æ–­åŠ å¼ºï¼Œå…ç–«æ¥ç§è¦†ç›–ç‡é€å¹´æé«˜ï¼Œå¤©èŠ±å¾—åˆ°æ¶ˆç­ï¼Œè„Šç®€ç°è´¨ç‚å·²æ¥è¿‘è¢«æ¶ˆç­ï¼Œè®¸å¤šä¼ æŸ“ç—…å¦‚ä¹™å‹è„‘ç‚ã€ç™½å–‰ã€ç™¾æ—¥å’³å’Œæ–°ç”Ÿå„¿ç ´ä¼¤é£ç­‰çš„å‘ç—…ç‡ä¹Ÿæ˜æ˜¾ä¸‹é™ã€‚ä½†æ˜¯æœ‰äº›ä¼ æŸ“ç—…å¦‚ç—…æ¯’æ€§è‚ç‚ã€å‡ºè¡€çƒ­ã€ç‹‚çŠ¬ç—…ã€ç»“æ ¸ç—…å’Œæ„ŸæŸ“æ€§è…¹æ³»ç­‰ä»ç„¶å¹¿æ³›å­˜åœ¨ï¼›æ–°å‘æ„ŸæŸ“ç—…åŒ…æ‹¬å˜å¼‚ç—…åŸä½“æ„ŸæŸ“å¤šæ¬¡å‡ºç°æµè¡Œï¼Œå¦‚ä¼ æŸ“æ€§éå…¸å‹è‚ºç‚åŠç”²å‹H1Nlæµæ„Ÿçš„è‚†è™ï¼Œå›½å¤–æµè¡Œçš„ä¼ æŸ“ç—…äº¦æœ‰å¯èƒ½ä¼ å…¥æˆ‘å›½ï¼›çƒˆæ€§ä¼ æŸ“ç—…è¿˜æœ‰å¯èƒ½æˆä¸ºç”Ÿç‰©ææ€–çš„ä¸»è¦å·¥å…·ã€‚å› æ­¤ï¼Œå¯¹ä¼ æŸ“ç—…çš„é˜²æ²»ç ”ç©¶ä»éœ€åŠ å¼ºã€‚ä¼ æŸ“ç—…ç ”ç©¶ä¸€ç›´æ˜¯å›½å®¶é‡å¤§ç§‘ç ”é¡¹ç›®å’Œè¯ç‰©å¼€å‘çš„é‡ç‚¹é¢†åŸŸï¼Œæ˜¯å½“å‰å›½å®¶é‡å¤§ç§‘æŠ€éœ€æ±‚ã€‚"}
{"text": "2ç¬¬ä¸€ç« æ€»è®º"}
{"text": "ç¥–å›½åŒ»å­¦å¯¹ä¼ æŸ“ç—…çš„é˜²æ²»æœ‰ä¸°å¯Œçš„ç»éªŒï¼Œæ·±å…¥å‘æ˜å’Œå‘å±•ç¥–å›½åŒ»å­¦ç ”ç©¶å°†å¯¹ä¸­è¥¿ç»“åˆé˜²æ²»ä¼ æŸ“ç—…å‘æŒ¥é‡è¦ä½œç”¨ã€‚"}
{"text": "ç¬¬ä¸€èŠ‚æ„ŸæŸ“ä¸å…ç–«â€”ã€æ„ŸæŸ“çš„æ¦‚å¿µ"}
```

## å°†å¦‚ä¸‹ python ä»£ç ä¿å­˜ä¸º train.py

```python
import json
from transformers import Trainer, TrainingArguments, AutoModel

# åŠ è½½é…ç½®
config = {
    "epochs": 10,
    "gradient_accumulation": 1,
    "lr": 0.00005,
    "model": "/mnt/model/base_model",
    "output": "/mnt/data/output",
    "resume_from_checkpoint": False,
    "reward_model": "",
    "save_total_limit": 10,
    "train_data_dir": "/mnt/data/train",
    "trainer": "pretrain",
    "valid_data_dir": "",
    "valid_data_split": 0.1,
    "warmup_ratio": 0.05
}

# åŠ è½½æ¨¡å‹
model = AutoModel.from_pretrained(config['model'])

# åŠ è½½æ•°æ®
from datasets import load_dataset
train_dataset = load_dataset('jsonl', data_files=f"{config['train_data_dir']}/train.jsonl")

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir=config['output'],
    num_train_epochs=config['epochs'],
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=int(len(train_dataset) * config['warmup_ratio']),
    weight_decay=0.01,
    logging_dir=f"{config['output']}/logs",
    learning_rate=config['lr'],
    save_total_limit=config['save_total_limit'],
    evaluation_strategy="epoch",
    load_best_model_at_end=False,
    metric_for_best_model="loss",
    greater_is_better=False,
    gradient_accumulation_steps=config['gradient_accumulation'],
    resume_from_checkpoint=config['resume_from_checkpoint'] if config['resume_from_checkpoint'] else None
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

## accelerate é…ç½®æ–‡ä»¶ accelerate_multiNode_config.yaml

master podï¼š

```yaml
compute_environment: LOCAL_MACHINE
debug: true
distributed_type: MULTI_GPU
downcast_bf16: 'no'
enable_cpu_affinity: false
gpu_ids: "0"
machine_rank: 0
main_process_ip: '10.233.97.242'
main_process_port: 29500
main_training_function: main
mixed_precision: fp16
num_machines: 2
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: [ ]
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

worker pod:

```yaml
debug: true
distributed_type: MULTI_GPU
downcast_bf16: 'no'
enable_cpu_affinity: false
gpu_ids: "0"
machine_rank: 1
main_process_ip: '10.233.97.242'
main_process_port: 29500
main_training_function: main
mixed_precision: fp16
num_machines: 2
num_processes: 2
rdzv_backend: static
same_network: true
tpu_env: [ ]
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
```

## è®­ç»ƒå¼€å§‹

åˆ†åˆ«åœ¨ master å’Œ worker å¯åŠ¨å‘½ä»¤

```shell
accelerate launch --config_file accelerate_multiNode_config.yaml train_main.py
```

![img_4.png](img_4.png)

![img_5.png](img_5.png)

```shell
root@ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-master-0:/app# ASCEND_RT_VISIBLE_DEVICES=0 accelerate launch --config_file accelerate_multiNode_config.yaml train.py
/usr/local/python3.10/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-02-18 02:51:10,526] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
^[[rank0]: Traceback (most recent call last):
[rank0]:   File "/app/train.py", line 57, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/transformers/trainer.py", line 2095, in _inner_training_loop
[rank0]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1339, in prepare
[rank0]:     result = tuple(
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1340, in <genexpr>
[rank0]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1215, in _prepare_one
[rank0]:     return self.prepare_model(obj, device_placement=device_placement)
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1469, in prepare_model
[rank0]:     model = torch.nn.parallel.DistributedDataParallel(
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 822, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/utils.py", line 286, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: RuntimeError: InnerRunOpApi:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:270 OPS function error: HcclAllgather, error code is 6
[rank0]: [ERROR] 2025-02-18-02:53:14 (PID:11691, Device:0, RankID:0) ERR01100 OPS call acl api failed.
[rank0]: EI0006: [PID: 11691] 2025-02-18-02:53:14.458.022 Getting socket times out. Reason: 1. The remote does not initiate a connect request. some NPUs in the cluster are abnormal.    2. The remote does not initiate a connect request because the collective communication operator is started too late or is not started by some NPU in the cluster.    3. The communication link is disconnected. (For example, the IP addresses are not on the same network segment or the TLS configurations are inconsistent.)
[rank0]:         Solution: 1. Check the rank service processes with other errors or no errors in the cluster.2. If this error is reported for all NPUs, check whether the time difference between the earliest and latest errors is greater than the connect timeout interval (120s by default). If so, adjust the timeout interval by using the HCCL_CONNECT_TIMEOUT environment variable.3. Check the connectivity of the communication link between nodes. (For details, see the TLS command and HCCN connectivity check examples.). For details:https://www.hiascend.com/document

E0218 02:53:20.777000 281473636308544 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 11691) of binary: /usr/local/python3.10/bin/python3.10
Traceback (most recent call last):
  File "/usr/local/python3.10/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-18_02:53:20
  host      : ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-master-0
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 11691)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[ERROR] 2025-02-18-02:53:20 (PID:11622, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
root@ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-master-0:/app#
```

```shell
root@ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-worker-0-0:/app# ASCEND_RT_VISIBLE_DEVICES=0 accelerate launch --config_file accelerate_multiNode_config.yaml train.py
/usr/local/python3.10/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-02-18 02:51:10,186] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/app/train.py", line 57, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/transformers/trainer.py", line 2095, in _inner_training_loop
[rank1]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1339, in prepare
[rank1]:     result = tuple(
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1340, in <genexpr>
[rank1]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1215, in _prepare_one
[rank1]:     return self.prepare_model(obj, device_placement=device_placement)
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/accelerator.py", line 1469, in prepare_model
[rank1]:     model = torch.nn.parallel.DistributedDataParallel(
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 822, in __init__
[rank1]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank1]:   File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/utils.py", line 286, in _verify_param_shape_across_processes
[rank1]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank1]: RuntimeError: InnerRunOpApi:build/CMakeFiles/torch_npu.dir/compiler_depend.ts:270 OPS function error: HcclAllgather, error code is 6
[rank1]: [ERROR] 2025-02-18-02:53:14 (PID:10897, Device:0, RankID:1) ERR01100 OPS call acl api failed.
[rank1]: EI0006: [PID: 10897] 2025-02-18-02:53:14.537.555 Getting socket times out. Reason: 1. The remote does not initiate a connect request. some NPUs in the cluster are abnormal.    2. The remote does not initiate a connect request because the collective communication operator is started too late or is not started by some NPU in the cluster.    3. The communication link is disconnected. (For example, the IP addresses are not on the same network segment or the TLS configurations are inconsistent.)
[rank1]:         Solution: 1. Check the rank service processes with other errors or no errors in the cluster.2. If this error is reported for all NPUs, check whether the time difference between the earliest and latest errors is greater than the connect timeout interval (120s by default). If so, adjust the timeout interval by using the HCCL_CONNECT_TIMEOUT environment variable.3. Check the connectivity of the communication link between nodes. (For details, see the TLS command and HCCN connectivity check examples.). For details:https://www.hiascend.com/document

E0218 02:53:21.288000 281473116186176 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 10897) of binary: /usr/local/python3.10/bin/python3.10
Traceback (most recent call last):
  File "/usr/local/python3.10/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/usr/local/python3.10/lib/python3.10/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/python3.10/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-18_02:53:21
  host      : ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-worker-0-0
  rank      : 1 (local_rank: 0)
  exitcode  : 1 (pid: 10897)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[ERROR] 2025-02-18-02:53:21 (PID:10829, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
root@ft-job-exec-id-cuhjsp6vdf5qkivmfhr0-llamafactory-worker-0-0:/app#
```
